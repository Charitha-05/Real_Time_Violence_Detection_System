{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Charitha-05/Real_Time_Violence_Detection_System/blob/main/Copy_of_Copy_of_Final_Violence_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYLUp3Qvb6X1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/'"
      ],
      "metadata": {
        "id": "jyiL6plIdJQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from transformers import TimesformerModel, TimesformerConfig\n",
        "from transformers import VideoMAEModel, VideoMAEConfig\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "1taP6AtWdMXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8b1edc7"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import transformer libraries\n",
        "from transformers import (\n",
        "    VideoMAEForVideoClassification,\n",
        "    VideoMAEImageProcessor,\n",
        "    TimesformerForVideoClassification,\n",
        "    AutoImageProcessor,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")"
      ],
      "metadata": {
        "id": "1HoVhMcCfApO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "wGJmygBCfGJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "45oHC97SfHbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_DIR = '/content/drive/MyDrive/Real Life Violence Dataset'  # path to video dataset\n",
        "IMG_SIZE = 224                            # height & width of frames\n",
        "NUM_FRAMES = 16                           # number of frames to sample per video\n",
        "SR = 16000                                # audio sampling rate (Hz)\n",
        "DURATION = 3                              # seconds of audio snippet to extract\n",
        "N_MELS = 128                               # mel-spectrogram bins for audio"
      ],
      "metadata": {
        "id": "B4Pj9lKWm4ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class_names = sorted(os.listdir(VIDEO_DIR))\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "for cls in class_names:\n",
        "    cls_dir = os.path.join(VIDEO_DIR, cls)\n",
        "    num_videos = len([f for f in os.listdir(cls_dir) if f.endswith(('.mp4', '.avi', '.mov'))])\n",
        "    print(f\"Class '{cls}' has {num_videos} videos\")\n"
      ],
      "metadata": {
        "id": "7RlHm7KUn5yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "total_size = 0\n",
        "\n",
        "for cls in os.listdir(VIDEO_DIR):\n",
        "    cls_dir = os.path.join(VIDEO_DIR, cls)\n",
        "    for f in os.listdir(cls_dir):\n",
        "        if f.endswith(('.mp4', '.avi', '.mov')):\n",
        "            file_path = os.path.join(cls_dir, f)\n",
        "            size_mb = os.path.getsize(file_path) / (1024 * 1024)  # bytes â†’ MB\n",
        "            total_size += size_mb\n",
        "            print(f\"{f} : {size_mb:.2f} MB\")\n",
        "\n",
        "print(f\"\\nTotal dataset size: {total_size:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "H5gRpMHrnyfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "OUTPUT_DIR = '/content/violence_dataset_split'\n",
        "\n",
        "classes = ['NonViolence','Violence']\n",
        "\n",
        "# Create folder structure\n",
        "for split in ['train','test']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(OUTPUT_DIR, split, cls), exist_ok=True)\n",
        "\n",
        "# Collect all video paths and labels\n",
        "video_paths, labels = [], []\n",
        "for idx, cls in enumerate(classes):\n",
        "    cls_dir = os.path.join(VIDEO_DIR, cls)\n",
        "    for f in os.listdir(cls_dir):\n",
        "        if f.endswith(('.mp4','.avi','.mov')):\n",
        "            video_paths.append(os.path.join(cls_dir, f))\n",
        "            labels.append(idx)\n",
        "\n",
        "# Split 80/20\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    video_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Copy files to new folders\n",
        "for path, label in zip(train_paths, train_labels):\n",
        "    cls = classes[label]\n",
        "    shutil.copy(path, os.path.join(OUTPUT_DIR, 'train', cls))\n",
        "\n",
        "for path, label in zip(test_paths, test_labels):\n",
        "    cls = classes[label]\n",
        "    shutil.copy(path, os.path.join(OUTPUT_DIR, 'test', cls))\n",
        "\n",
        "print(\"âœ… Dataset split into train/test folders completed.\")\n"
      ],
      "metadata": {
        "id": "j-_sVlTw0Rdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Dataset directoryife Violence Dataset'\n",
        "\n",
        "# Define class names\n",
        "classes = ['NonViolence', 'Violence']\n",
        "\n",
        "# Collect video paths and labels\n",
        "video_paths, labels = [], []\n",
        "for idx, cls in enumerate(classes):\n",
        "    cls_dir = os.path.join(VIDEO_DIR, cls)\n",
        "    for f in os.listdir(cls_dir):\n",
        "        if f.lower().endswith(('.mp4', '.avi', '.mov')):\n",
        "            video_paths.append(os.path.join(cls_dir, f))\n",
        "            labels.append(idx)\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "    video_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to DataFrames for easier viewing\n",
        "train_df = pd.DataFrame({'path': train_paths, 'label': train_labels})\n",
        "test_df = pd.DataFrame({'path': test_paths, 'label': test_labels})\n",
        "\n",
        "# Map numeric labels to class names\n",
        "train_df['class_name'] = train_df['label'].map({i: c for i, c in enumerate(classes)})\n",
        "test_df['class_name'] = test_df['label'].map({i: c for i, c in enumerate(classes)})\n",
        "\n",
        "# Print summary\n",
        "print(\"=\"*60)\n",
        "print(\"âœ… Dataset Split Summary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total Samples: {len(video_paths)}\")\n",
        "print(f\"Train Samples: {len(train_paths)}\")\n",
        "print(f\"Test Samples:  {len(test_paths)}\\n\")\n",
        "\n",
        "print(\"ðŸ“Š Class Distribution:\")\n",
        "print(\"Train set:\")\n",
        "print(train_df['class_name'].value_counts())\n",
        "print(\"\\nTest set:\")\n",
        "print(test_df['class_name'].value_counts())"
      ],
      "metadata": {
        "id": "OYDAL_E3zsJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "from IPython.display import HTML, display\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# --- Helper: Convert a video file to HTML5 playable format (Base64) ---\n",
        "def video_to_html(path, width=400):\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            data = f.read()\n",
        "        b64 = base64.b64encode(data).decode('utf-8')\n",
        "        return f'<video width=\"{width}\" controls><source src=\"data:video/mp4;base64,{b64}\" type=\"video/mp4\"></video>'\n",
        "    except Exception as e:\n",
        "        return f\"<p style='color:red;'>Error loading video: {e}</p>\"\n",
        "\n",
        "# --- Helper: Pick one random video path for a given class ---\n",
        "def pick_video(df, classname):\n",
        "    subset = df[df['class_name'] == classname]\n",
        "    if len(subset) == 0:\n",
        "        return None\n",
        "    return random.choice(subset['path'].tolist())\n",
        "\n",
        "# --- Pick sample videos from each class/split ---\n",
        "train_violence_path = pick_video(train_df, 'Violence')\n",
        "train_nonviolence_path = pick_video(train_df, 'NonViolence')\n",
        "test_violence_path = pick_video(test_df, 'Violence')\n",
        "test_nonviolence_path = pick_video(test_df, 'NonViolence')\n",
        "\n",
        "# --- Convert each to HTML5 playable Base64 ---\n",
        "train_violence_html = video_to_html(train_violence_path)\n",
        "train_nonviolence_html = video_to_html(train_nonviolence_path)\n",
        "test_violence_html = video_to_html(test_violence_path)\n",
        "test_nonviolence_html = video_to_html(test_nonviolence_path)\n",
        "\n",
        "# --- Build final HTML grid layout ---\n",
        "html_content = f\"\"\"\n",
        "<table style=\"width:100%; text-align:center;\">\n",
        "  <tr>\n",
        "    <td>{train_violence_html}<br><b>Train - Violence</b></td>\n",
        "    <td>{train_nonviolence_html}<br><b>Train - NonViolence</b></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>{test_violence_html}<br><b>Test - Violence</b></td>\n",
        "    <td>{test_nonviolence_html}<br><b>Test - NonViolence</b></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "# --- Display in notebook ---\n",
        "display(HTML(html_content))\n",
        "\n",
        "# --- Print selected paths for clarity ---\n",
        "print(\"ðŸŽ¬ Selected video paths:\")\n",
        "print(f\"Train - Violence:     {train_violence_path}\")\n",
        "print(f\"Train - NonViolence:  {train_nonviolence_path}\")\n",
        "print(f\"Test - Violence:      {test_violence_path}\")\n",
        "print(f\"Test - NonViolence:   {test_nonviolence_path}\")\n"
      ],
      "metadata": {
        "id": "rZDvUCtIzwrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install decord"
      ],
      "metadata": {
        "id": "vk1Yambvzzl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "import torchvision.transforms as T\n",
        "\n",
        "decord.bridge.set_bridge('torch')  # output videos as torch tensors\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, num_frames=16, frame_size=(224,224)):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.num_frames = num_frames\n",
        "        self.frame_size = frame_size\n",
        "\n",
        "        # Transform: Resize + Normalize\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize(frame_size),  # resize each frame\n",
        "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        vr = VideoReader(path, ctx=cpu(0))\n",
        "        total_frames = len(vr)\n",
        "\n",
        "        # Sample num_frames evenly across the video\n",
        "        frame_indices = torch.linspace(0, total_frames-1, self.num_frames).long()\n",
        "        frames = vr.get_batch(frame_indices)  # (num_frames, H, W, C), dtype=torch.uint8\n",
        "\n",
        "        # Convert to float and scale to [0,1]\n",
        "        frames = frames.float() / 255.0\n",
        "\n",
        "        # Permute to (num_frames, C, H, W)\n",
        "        frames = frames.permute(0, 3, 1, 2)\n",
        "\n",
        "        # Apply transform to each frame\n",
        "        frames = torch.stack([self.transform(frame) for frame in frames])\n",
        "\n",
        "        return frames, label\n"
      ],
      "metadata": {
        "id": "PJ260nTirwbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = VideoDataset(train_paths, train_labels, num_frames=16, frame_size=(224,224))\n",
        "test_dataset = VideoDataset(test_paths, test_labels, num_frames=16, frame_size=(224,224))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Check shapes\n",
        "for videos, labels in train_loader:\n",
        "    print(videos.shape, labels)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "_YKBOgJirzfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers timm\n"
      ],
      "metadata": {
        "id": "RQ1Uooqjr_cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VideoMAEForVideoClassification, VideoMAEFeatureExtractor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 2  # Violence / NonViolence\n",
        "\n",
        "# Load pretrained VideoMAE\n",
        "video_model = VideoMAEForVideoClassification.from_pretrained(\n",
        "    \"MCG-NJU/videomae-base\",\n",
        "    num_labels=num_classes\n",
        ")\n",
        "video_model.to(device)\n"
      ],
      "metadata": {
        "id": "AmLhbBSTsOGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(video_model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "AuomDVp1sSm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5  # adjust as needed\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    video_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for videos, labels in train_loader:\n",
        "        videos = videos.to(device)          # (B, F, C, H, W)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = video_model(videos).logits     # forward pass\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "Wj0E3doLsU7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for videos, labels in test_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = video_model(videos).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100*correct/total:.2f}%\")"
      ],
      "metadata": {
        "id": "azNYU_-ZN2ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SAVE VIDEO MODEL ===\n",
        "video_model_save_dir = \"/content/drive/MyDrive/video_violence_detection_model\"\n",
        "\n",
        "video_model.save_pretrained(video_model_save_dir)\n",
        "print(f\"âœ… Video model saved successfully at: {video_model_save_dir}\")\n"
      ],
      "metadata": {
        "id": "OnbL0qt_LqQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Pick a sample video (e.g., first in test set)\n",
        "sample_video_path = test_paths[0]\n",
        "sample_label = test_labels[0]\n",
        "\n",
        "# Create a single-sample dataset and loader\n",
        "sample_dataset = VideoDataset([sample_video_path], [sample_label], num_frames=16, frame_size=(224,224))\n",
        "sample_loader = torch.utils.data.DataLoader(sample_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "video_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video, label in sample_loader:\n",
        "        video = video.to(device)          # (1, F, C, H, W)\n",
        "        label = label.to(device)\n",
        "\n",
        "        outputs = video_model(video).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        print(f\"True label: {label.item()}, Predicted: {predicted.item()}\")\n"
      ],
      "metadata": {
        "id": "uXkjUCqx6dUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Provide the video path directly\n",
        "sample_video_path = \"/content/violence_dataset_split/test/NonViolence/NV_120.mp4\"  # replace with your actual path\n",
        "# Provide the true label manually (0=NonViolence, 1=Violence)\n",
        "sample_label = 0\n",
        "\n",
        "# Create a single-sample dataset and loader\n",
        "sample_dataset = VideoDataset([sample_video_path], [sample_label], num_frames=16, frame_size=(224,224))\n",
        "sample_loader = torch.utils.data.DataLoader(sample_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "video_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video, label in sample_loader:\n",
        "        video = video.to(device)          # (1, F, C, H, W)\n",
        "        label = label.to(device)\n",
        "\n",
        "        outputs = video_model(video).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        print(f\"Video Path: {sample_video_path}\")\n",
        "        print(f\"True label: {label.item()} ({classes[label.item()]}), Predicted: {predicted.item()} ({classes[predicted.item()]})\")\n"
      ],
      "metadata": {
        "id": "0zftVQNZ6gAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Provide the video path directly\n",
        "sample_video_path = \"/content/violence_dataset_split/test/Violence/V_131.mp4\"  # replace with your actual path\n",
        "# Provide the true label manually (0=NonViolence, 1=Violence)\n",
        "sample_label = 1\n",
        "\n",
        "# Create a single-sample dataset and loader\n",
        "sample_dataset = VideoDataset([sample_video_path], [sample_label], num_frames=16, frame_size=(224,224))\n",
        "sample_loader = torch.utils.data.DataLoader(sample_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "video_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video, label in sample_loader:\n",
        "        video = video.to(device)          # (1, F, C, H, W)\n",
        "        label = label.to(device)\n",
        "\n",
        "        outputs = video_model(video).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        print(f\"Video Path: {sample_video_path}\")\n",
        "        print(f\"True label: {label.item()} ({classes[label.item()]}), Predicted: {predicted.item()} ({classes[predicted.item()]})\")\n"
      ],
      "metadata": {
        "id": "bDQEal6d7VTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "import numpy as np\n",
        "\n",
        "# Provide the video path directly\n",
        "sample_video_path = \"/content/violence_dataset_split/test/Violence/V_14.mp4\"  # replace with your actual path\n",
        "sample_label = 1  # 0=NonViolence, 1=Violence\n",
        "\n",
        "# Create a single-sample dataset and loader\n",
        "sample_dataset = VideoDataset([sample_video_path], [sample_label], num_frames=16, frame_size=(224,224))\n",
        "sample_loader = torch.utils.data.DataLoader(sample_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "video_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video, label in sample_loader:\n",
        "        video = video.to(device)          # (1, F, C, H, W)\n",
        "        label = label.to(device)\n",
        "\n",
        "        outputs = video_model(video).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        print(f\"Video Path: {sample_video_path}\")\n",
        "        print(f\"True label: {label.item()} ({classes[label.item()]}), Predicted: {predicted.item()} ({classes[predicted.item()]})\")\n",
        "\n",
        "# --- Display the video in Colab ---\n",
        "def show_video(path):\n",
        "    mp4 = open(path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "        <video width=480 controls>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    \"\"\"))\n",
        "\n",
        "show_video(sample_video_path)\n"
      ],
      "metadata": {
        "id": "t0cv_Z8C8dim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64encode\n",
        "import numpy as np\n",
        "\n",
        "# Provide the video path directly\n",
        "sample_video_path = \"/content/violence_dataset_split/test/NonViolence/NV_19.mp4\"  # replace with your actual path\n",
        "sample_label = 0  # 0=NonViolence, 1=Violence\n",
        "\n",
        "# Create a single-sample dataset and loader\n",
        "sample_dataset = VideoDataset([sample_video_path], [sample_label], num_frames=16, frame_size=(224,224))\n",
        "sample_loader = torch.utils.data.DataLoader(sample_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Put model in evaluation mode\n",
        "video_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for video, label in sample_loader:\n",
        "        video = video.to(device)          # (1, F, C, H, W)\n",
        "        label = label.to(device)\n",
        "\n",
        "        outputs = video_model(video).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        print(f\"Video Path: {sample_video_path}\")\n",
        "        print(f\"True label: {label.item()} ({classes[label.item()]}), Predicted: {predicted.item()} ({classes[predicted.item()]})\")\n",
        "\n",
        "# --- Display the video in Colab ---\n",
        "def show_video(path):\n",
        "    mp4 = open(path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f\"\"\"\n",
        "        <video width=480 controls>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    \"\"\"))\n",
        "\n",
        "show_video(sample_video_path)\n"
      ],
      "metadata": {
        "id": "TLwWdqPaMDvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "video_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for videos, labels in test_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = video_model(videos).logits\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Visualize\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uuD21_Ue7c9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# === Input: folder containing audio files ===\n",
        "audio_dir = \"/content/drive/MyDrive/archive (11)/audios_VSD/audios_VSD\"  # change to your folder path\n",
        "\n",
        "# Supported audio formats\n",
        "valid_extensions = ('.mp3', '.wav', '.ogg', '.flac', '.m4a')\n",
        "\n",
        "# List to store results\n",
        "audio_durations = []\n",
        "\n",
        "# Loop through all files in the directory\n",
        "for filename in os.listdir(audio_dir):\n",
        "    if filename.lower().endswith(valid_extensions):\n",
        "        file_path = os.path.join(audio_dir, filename)\n",
        "        try:\n",
        "            audio = AudioSegment.from_file(file_path)\n",
        "            duration = len(audio) / 1000.0  # milliseconds â†’ seconds\n",
        "            audio_durations.append((filename, duration))\n",
        "            print(f\"{filename}: {duration:.2f} seconds\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Could not read {filename}: {e}\")\n",
        "\n",
        "# Optionally save results to CSV\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(audio_durations, columns=[\"Filename\", \"Duration_seconds\"])\n",
        "output_csv = \"/content/audio_durations.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Durations saved to: {output_csv}\")\n"
      ],
      "metadata": {
        "id": "xkSaBma5rgRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "# === CONFIG ===\n",
        "EXCEL_PATH = \"/content/drive/MyDrive/audio dataset/VSD.xlsx\"\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/audio dataset/audios_VSD/audios_VSD\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/split_audios\"\n",
        "\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"Violence\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"NonViolence\"), exist_ok=True)\n",
        "\n",
        "# === LOAD EXCEL ===\n",
        "df = pd.read_excel(EXCEL_PATH)\n",
        "\n",
        "# Clean column names\n",
        "df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "# Expecting columns: file_segment_name, violence_start, violence_end\n",
        "if not all(col in df.columns for col in ['file_segment_name', 'violence_start', 'violence_end']):\n",
        "    raise ValueError(\"Excel must contain columns: File_segment_name, Violence_start, Violence_end\")\n",
        "\n",
        "# Group by each audio file\n",
        "for file_name, group in df.groupby('file_segment_name'):\n",
        "    # Add file extension (assuming .wav)\n",
        "    audio_path = os.path.join(AUDIO_DIR, f\"{file_name}.wav\")\n",
        "    if not os.path.exists(audio_path):\n",
        "        print(f\"âš ï¸ Missing audio file: {audio_path}\")\n",
        "        continue\n",
        "\n",
        "    # Load full audio\n",
        "    y, sr = librosa.load(audio_path, sr=16000)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "\n",
        "    # Collect violent segments\n",
        "    violent_ranges = [\n",
        "        (max(0, float(row.violence_start)), min(duration, float(row.violence_end)))\n",
        "        for _, row in group.iterrows()\n",
        "        if not np.isnan(row.violence_start) and not np.isnan(row.violence_end)\n",
        "    ]\n",
        "    violent_ranges.sort(key=lambda x: x[0])\n",
        "\n",
        "    # === Save violent segments ===\n",
        "    for i, (start, end) in enumerate(violent_ranges):\n",
        "        seg = y[int(start * sr): int(end * sr)]\n",
        "        out_path = os.path.join(OUTPUT_DIR, \"Violence\", f\"{file_name}_V{i}.wav\")\n",
        "        sf.write(out_path, seg, sr)\n",
        "\n",
        "    # === Find non-violent ranges ===\n",
        "    non_violent_ranges = []\n",
        "    prev_end = 0.0\n",
        "    for start, end in violent_ranges:\n",
        "        if start - prev_end > 1.0:  # ignore short gaps <1s\n",
        "            non_violent_ranges.append((prev_end, start))\n",
        "        prev_end = end\n",
        "    if duration - prev_end > 1.0:\n",
        "        non_violent_ranges.append((prev_end, duration))\n",
        "\n",
        "    # === Save non-violent segments ===\n",
        "    for i, (start, end) in enumerate(non_violent_ranges):\n",
        "        seg = y[int(start * sr): int(end * sr)]\n",
        "        out_path = os.path.join(OUTPUT_DIR, \"NonViolence\", f\"{file_name}_NV{i}.wav\")\n",
        "        sf.write(out_path, seg, sr)\n",
        "\n",
        "    print(f\"âœ… Processed {file_name}: {len(violent_ranges)} violent, {len(non_violent_ranges)} non-violent segments\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ All files processed successfully!\")"
      ],
      "metadata": {
        "id": "h-rDhxcCr4Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/split_audios\"\n",
        "\n",
        "violence = [(f, 1) for f in glob.glob(os.path.join(OUTPUT_DIR, \"Violence\", \"*.wav\"))]\n",
        "nonviolence = [(f, 0) for f in glob.glob(os.path.join(OUTPUT_DIR, \"NonViolence\", \"*.wav\"))]\n",
        "\n",
        "df = pd.DataFrame(violence + nonviolence, columns=[\"filepath\", \"label\"])\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df.to_csv(os.path.join(OUTPUT_DIR, \"dataset.csv\"), index=False)\n",
        "\n",
        "print(\"âœ… Created dataset.csv with\", len(df), \"samples\")"
      ],
      "metadata": {
        "id": "c-8nbuUDsBBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# === CONFIG ===\n",
        "VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/Violence\"       # <-- change to your actual folder\n",
        "NON_VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/NonViolence\" # <-- change to your actual folder\n",
        "\n",
        "# === GET RANDOM FILES ===\n",
        "violent_files = [f for f in os.listdir(VIOLENT_DIR) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "non_violent_files = [f for f in os.listdir(NON_VIOLENT_DIR) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "\n",
        "if not violent_files or not non_violent_files:\n",
        "    raise ValueError(\"Make sure both folders have audio files!\")\n",
        "\n",
        "rand_violent = random.choice(violent_files)\n",
        "rand_non_violent = random.choice(non_violent_files)\n",
        "\n",
        "print(f\"ðŸŽ§ Violent sample: {rand_violent}\")\n",
        "display(Audio(os.path.join(VIOLENT_DIR, rand_violent)))\n",
        "\n",
        "print(f\"\\nðŸŽµ Non-Violent sample: {rand_non_violent}\")\n",
        "display(Audio(os.path.join(NON_VIOLENT_DIR, rand_non_violent)))\n"
      ],
      "metadata": {
        "id": "xff4aoxcsEGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/Violence\"       # <-- change to your actual folder\n",
        "NON_VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/NonViolence\" # <-- change to your actual folder\n",
        "\n",
        "\n",
        "# === COUNT FILES ===\n",
        "violent_files = [f for f in os.listdir(VIOLENT_DIR) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "non_violent_files = [f for f in os.listdir(NON_VIOLENT_DIR) if f.endswith(('.wav', '.mp3', '.ogg'))]\n",
        "\n",
        "violent_count = len(violent_files)\n",
        "non_violent_count = len(non_violent_files)\n",
        "\n",
        "print(f\"Total Violent samples: {violent_count}\")\n",
        "print(f\"Total Non-Violent samples: {non_violent_count}\")"
      ],
      "metadata": {
        "id": "7oJDc4SGsHH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install audiomentations pydub scikit-learn"
      ],
      "metadata": {
        "id": "9LCpREH7sLdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
        "from pydub import AudioSegment\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === CONFIG ===\n",
        "VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/Violence\"\n",
        "NON_VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/NonViolence\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/processed_audios\"\n",
        "\n",
        "# Create output folders\n",
        "for folder in [\"train/Violence\", \"train/NonViolence\", \"test/Violence\", \"test/NonViolence\"]:\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, folder), exist_ok=True)\n",
        "\n",
        "# === PARAMETERS ===\n",
        "TARGET_COUNT = 235   # âœ… changed target per class to 235\n",
        "TEST_SIZE = 0.2      # 80/20 split\n",
        "RANDOM_SEED = 42\n",
        "SHIFT_PROB = 0.5     # probability to apply manual time-shift\n",
        "MAX_SHIFT_SECONDS = 0.5  # max shift in seconds (Â±)\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# === AUGMENTATION PIPELINE (audiomentations) ===\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    TimeStretch(min_rate=0.85, max_rate=1.2, p=0.5),\n",
        "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
        "])\n",
        "\n",
        "def manual_time_shift(samples: np.ndarray, sample_rate: int, max_shift_seconds: float):\n",
        "    \"\"\"Shift audio by rolling the array within Â±max_shift_seconds.\"\"\"\n",
        "    if max_shift_seconds <= 0:\n",
        "        return samples\n",
        "    max_shift_samples = int(max_shift_seconds * sample_rate)\n",
        "    if max_shift_samples <= 0:\n",
        "        return samples\n",
        "    shift = random.randint(-max_shift_samples, max_shift_samples)\n",
        "    return np.roll(samples, shift)\n",
        "\n",
        "def augment_class(class_name, class_dir, target_count):\n",
        "    \"\"\"Augment a class directory up to target_count files\"\"\"\n",
        "    files = [os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
        "             if f.lower().endswith(('.wav', '.mp3', '.ogg', '.flac'))]\n",
        "    print(f\"\\nðŸŽ§ {class_name}: {len(files)} original samples\")\n",
        "\n",
        "    if len(files) < target_count:\n",
        "        needed = target_count - len(files)\n",
        "        augmented_files = []\n",
        "        for i in range(needed):\n",
        "            src_file = random.choice(files)\n",
        "            audio = AudioSegment.from_file(src_file)\n",
        "            # convert to mono for simplicity\n",
        "            if audio.channels > 1:\n",
        "                audio = audio.set_channels(1)\n",
        "\n",
        "            samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
        "            sample_rate = audio.frame_rate\n",
        "\n",
        "            # Normalize to [-1, 1]\n",
        "            max_val = np.max(np.abs(samples)) + 1e-9\n",
        "            samples = samples / max_val\n",
        "\n",
        "            # Apply audiomentations\n",
        "            augmented = augment(samples=samples, sample_rate=sample_rate)\n",
        "\n",
        "            # Optionally apply manual time shift\n",
        "            if random.random() < SHIFT_PROB:\n",
        "                augmented = manual_time_shift(augmented, sample_rate, MAX_SHIFT_SECONDS)\n",
        "\n",
        "            # Convert back to int16\n",
        "            augmented = np.clip(augmented, -1.0, 1.0)\n",
        "            augmented_int16 = (augmented * 32767).astype(np.int16)\n",
        "\n",
        "            # Save augmented file\n",
        "            augmented_audio = AudioSegment(\n",
        "                augmented_int16.tobytes(),\n",
        "                frame_rate=sample_rate,\n",
        "                sample_width=2,\n",
        "                channels=1\n",
        "            )\n",
        "\n",
        "            base_name = os.path.splitext(os.path.basename(src_file))[0]\n",
        "            aug_name = f\"aug_{i}_{base_name}.wav\"\n",
        "            aug_path = os.path.join(class_dir, aug_name)\n",
        "            augmented_audio.export(aug_path, format=\"wav\")\n",
        "            augmented_files.append(aug_path)\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"  -> created {i+1}/{needed} augmented samples for {class_name}\")\n",
        "\n",
        "        files += augmented_files\n",
        "\n",
        "    print(f\"âœ… After augmentation: {len(files)} total samples for {class_name}\")\n",
        "    return files\n",
        "\n",
        "# === RUN AUGMENTATION FOR BOTH CLASSES ===\n",
        "violent_files = augment_class(\"Violence\", VIOLENT_DIR, TARGET_COUNT)\n",
        "non_violent_files = augment_class(\"NonViolence\", NON_VIOLENT_DIR, TARGET_COUNT)\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "v_train, v_test = train_test_split(violent_files, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "nv_train, nv_test = train_test_split(non_violent_files, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "\n",
        "# === COPY FILES TO DESTINATION ===\n",
        "def copy_files(file_list, dest_folder):\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "    for f in file_list:\n",
        "        shutil.copy(f, dest_folder)\n",
        "\n",
        "copy_files(v_train, os.path.join(OUTPUT_DIR, \"train/Violence\"))\n",
        "copy_files(v_test, os.path.join(OUTPUT_DIR, \"test/Violence\"))\n",
        "copy_files(nv_train, os.path.join(OUTPUT_DIR, \"train/NonViolence\"))\n",
        "copy_files(nv_test, os.path.join(OUTPUT_DIR, \"test/NonViolence\"))\n",
        "\n",
        "# === DONE ===\n",
        "print(\"\\nâœ… Data augmentation and processing complete!\")\n",
        "print(f\"Train/Test split = {1 - TEST_SIZE:.0%}/{TEST_SIZE:.0%}\")\n",
        "print(f\"Training Violent: {len(v_train)} | Testing Violent: {len(v_test)}\")\n",
        "print(f\"Training Non-Violent: {len(nv_train)} | Testing Non-Violent: {len(nv_test)}\")\n",
        "print(f\"Files saved to: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "WT2DH0sHsTE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
        "from pydub import AudioSegment\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === CONFIG ===\n",
        "VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/Violence\"\n",
        "NON_VIOLENT_DIR = \"/content/drive/MyDrive/split_audios/NonViolence\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/processed_audios\"\n",
        "\n",
        "# Create output folders\n",
        "for folder in [\"train/Violence\", \"train/NonViolence\", \"test/Violence\", \"test/NonViolence\"]:\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, folder), exist_ok=True)\n",
        "\n",
        "# === PARAMETERS ===\n",
        "TARGET_COUNT = 255   # target count per class\n",
        "TEST_SIZE = 0.2      # 80/20 split\n",
        "RANDOM_SEED = 42\n",
        "SHIFT_PROB = 0.5     # probability to apply manual time-shift\n",
        "MAX_SHIFT_SECONDS = 0.5  # max shift in seconds (Â±)\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# === AUGMENTATION PIPELINE (audiomentations) ===\n",
        "augment = Compose([\n",
        "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
        "    TimeStretch(min_rate=0.85, max_rate=1.2, p=0.5),\n",
        "    PitchShift(min_semitones=-2, max_semitones=2, p=0.5),\n",
        "])\n",
        "\n",
        "def manual_time_shift(samples: np.ndarray, sample_rate: int, max_shift_seconds: float):\n",
        "    \"\"\"\n",
        "    Shift audio by rolling the array. Shift amount chosen uniformly from\n",
        "    [-max_shift_seconds, +max_shift_seconds].\n",
        "    \"\"\"\n",
        "    if max_shift_seconds <= 0:\n",
        "        return samples\n",
        "    max_shift_samples = int(max_shift_seconds * sample_rate)\n",
        "    if max_shift_samples <= 0:\n",
        "        return samples\n",
        "    shift = random.randint(-max_shift_samples, max_shift_samples)\n",
        "    return np.roll(samples, shift)\n",
        "\n",
        "def augment_class(class_name, class_dir, target_count):\n",
        "    \"\"\"Augment a class directory up to target_count files\"\"\"\n",
        "    files = [os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
        "             if f.lower().endswith(('.wav', '.mp3', '.ogg', '.flac'))]\n",
        "    print(f\"\\nðŸŽ§ {class_name}: {len(files)} original samples\")\n",
        "\n",
        "    if len(files) < target_count:\n",
        "        needed = target_count - len(files)\n",
        "        augmented_files = []\n",
        "        for i in range(needed):\n",
        "            src_file = random.choice(files)\n",
        "            audio = AudioSegment.from_file(src_file)\n",
        "            # convert to mono for simplicity (avoids channel interleaving issues)\n",
        "            if audio.channels > 1:\n",
        "                audio = audio.set_channels(1)\n",
        "\n",
        "            samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
        "            sample_rate = audio.frame_rate\n",
        "\n",
        "            # Normalize to [-1, 1]\n",
        "            max_val = np.max(np.abs(samples)) + 1e-9\n",
        "            samples = samples / max_val\n",
        "\n",
        "            # audiomentations expects float32 in [-1,1]\n",
        "            augmented = augment(samples=samples, sample_rate=sample_rate)\n",
        "\n",
        "            # optionally apply manual time shift\n",
        "            if random.random() < SHIFT_PROB:\n",
        "                augmented = manual_time_shift(augmented, sample_rate, MAX_SHIFT_SECONDS)\n",
        "\n",
        "            # clip and convert to int16\n",
        "            augmented = np.clip(augmented, -1.0, 1.0)\n",
        "            augmented_int16 = (augmented * 32767).astype(np.int16)\n",
        "\n",
        "            # Convert back to AudioSegment and export\n",
        "            augmented_audio = AudioSegment(\n",
        "                augmented_int16.tobytes(),\n",
        "                frame_rate=sample_rate,\n",
        "                sample_width=2,   # int16 -> 2 bytes\n",
        "                channels=1\n",
        "            )\n",
        "\n",
        "            # unique name to avoid collisions\n",
        "            base_name = os.path.splitext(os.path.basename(src_file))[0]\n",
        "            aug_name = f\"aug_{i}_{base_name}.wav\"\n",
        "            aug_path = os.path.join(class_dir, aug_name)\n",
        "            augmented_audio.export(aug_path, format=\"wav\")\n",
        "            augmented_files.append(aug_path)\n",
        "\n",
        "            if (i + 1) % 20 == 0:\n",
        "                print(f\"  -> created {i+1}/{needed} augmented samples for {class_name}\")\n",
        "\n",
        "        files += augmented_files\n",
        "\n",
        "    print(f\"âœ… After augmentation: {len(files)} total samples for {class_name}\")\n",
        "    return files\n",
        "\n",
        "# === RUN AUGMENTATION FOR BOTH CLASSES ===\n",
        "violent_files = augment_class(\"Violence\", VIOLENT_DIR, TARGET_COUNT)\n",
        "non_violent_files = augment_class(\"NonViolence\", NON_VIOLENT_DIR, TARGET_COUNT)\n",
        "\n",
        "# === SPLIT DATA ===\n",
        "v_train, v_test = train_test_split(violent_files, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "nv_train, nv_test = train_test_split(non_violent_files, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "\n",
        "# === COPY FILES TO DESTINATION ===\n",
        "def copy_files(file_list, dest_folder):\n",
        "    os.makedirs(dest_folder, exist_ok=True)\n",
        "    for f in file_list:\n",
        "        shutil.copy(f, dest_folder)\n",
        "\n",
        "copy_files(v_train, os.path.join(OUTPUT_DIR, \"train/Violence\"))\n",
        "copy_files(v_test, os.path.join(OUTPUT_DIR, \"test/Violence\"))\n",
        "copy_files(nv_train, os.path.join(OUTPUT_DIR, \"train/NonViolence\"))\n",
        "copy_files(nv_test, os.path.join(OUTPUT_DIR, \"test/NonViolence\"))\n",
        "\n",
        "# === DONE ===\n",
        "print(\"\\nâœ… Data augmentation and processing complete!\")\n",
        "print(f\"Train/Test split = {1 - TEST_SIZE:.0%}/{TEST_SIZE:.0%}\")\n",
        "print(f\"Training Violent: {len(v_train)} | Testing Violent: {len(v_test)}\")\n",
        "print(f\"Training Non-Violent: {len(nv_train)} | Testing Non-Violent: {len(nv_test)}\")\n",
        "print(f\"Files saved to: {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "id": "zOHwXhS4sT6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Install required packages ===\n",
        "!pip install torch torchaudio torchvision --quiet\n",
        "!pip install transformers datasets librosa soundfile tqdm --quiet\n",
        "!pip install accelerate evaluate --quiet"
      ],
      "metadata": {
        "id": "8mokgXPesjZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
        "\n",
        "# Example: use Audio Spectrogram Transformer (AST)\n",
        "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "print(\"âœ… Transformer model loaded:\", model_name)"
      ],
      "metadata": {
        "id": "JC75QneBslgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoProcessor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "7IGASjtqsoi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "\n",
        "# Load pretrained processor\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Load pretrained model first\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n",
        "\n",
        "# Get hidden size from config (used for classifier input)\n",
        "hidden_size = model.classifier.dense.in_features\n",
        "\n",
        "# Replace classifier with new 2-class head\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_size, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 2)\n",
        ")\n",
        "\n",
        "# Update model config for 2 labels\n",
        "label2id = {\"NonViolence\": 0, \"Violence\": 1}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "model.config.num_labels = 2\n",
        "model.config.label2id = label2id\n",
        "model.config.id2label = id2label\n",
        "\n",
        "print(\"âœ… Replaced AST classifier head for 2-class fine-tuning.\")\n",
        "print(model.classifier)"
      ],
      "metadata": {
        "id": "i-QZ0Mi7ssob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, librosa\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def load_audio_split(base_dir):\n",
        "    data_train = {\"audio\": [], \"label\": []}\n",
        "    data_test = {\"audio\": [], \"label\": []}\n",
        "\n",
        "    # Loop through both class folders\n",
        "    for label_name in [\"NonViolence\", \"Violence\"]:\n",
        "        # Corrected paths\n",
        "        train_dir = os.path.join(base_dir, \"train\", label_name)\n",
        "        test_dir = os.path.join(base_dir, \"test\", label_name)\n",
        "\n",
        "        # === TRAIN FILES ===\n",
        "        if os.path.exists(train_dir):\n",
        "            files = [f for f in os.listdir(train_dir) if f.endswith((\".wav\", \".mp3\", \".ogg\"))]\n",
        "            # limit each class to 132 samples\n",
        "            files = files[:132] if len(files) > 132 else files\n",
        "            for f in files:\n",
        "                data_train[\"audio\"].append(os.path.join(train_dir, f))\n",
        "                data_train[\"label\"].append(model.config.label2id[label_name])\n",
        "\n",
        "        # === TEST FILES ===\n",
        "        if os.path.exists(test_dir):\n",
        "            for f in os.listdir(test_dir):\n",
        "                if f.endswith((\".wav\", \".mp3\", \".ogg\")):\n",
        "                    data_test[\"audio\"].append(os.path.join(test_dir, f))\n",
        "                    data_test[\"label\"].append(model.config.label2id[label_name])\n",
        "\n",
        "    return data_train, data_test\n",
        "\n",
        "\n",
        "# âœ… Correct base directory (not /train)\n",
        "base_dir = \"/content/drive/MyDrive/processed_audios\"\n",
        "\n",
        "train_data, test_data = load_audio_split(base_dir)\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "test_dataset = Dataset.from_dict(test_data)\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "print(f\"âœ… Loaded {len(train_dataset)} train and {len(test_dataset)} test samples.\")\n"
      ],
      "metadata": {
        "id": "dNL_c4kjsvvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, librosa\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def load_audio_split(base_dir):\n",
        "    data_train = {\"audio\": [], \"label\": []}\n",
        "    data_test = {\"audio\": [], \"label\": []}\n",
        "\n",
        "    for label_name in [\"NonViolence\", \"Violence\"]:\n",
        "        train_dir = os.path.join(base_dir, \"train\", label_name)\n",
        "        test_dir = os.path.join(base_dir, \"test\", label_name)\n",
        "\n",
        "        for f in os.listdir(train_dir):\n",
        "            if f.endswith((\".wav\", \".mp3\", \".ogg\")):\n",
        "                data_train[\"audio\"].append(os.path.join(train_dir, f))\n",
        "                data_train[\"label\"].append(model.config.label2id[label_name])\n",
        "\n",
        "        for f in os.listdir(test_dir):\n",
        "            if f.endswith((\".wav\", \".mp3\", \".ogg\")):\n",
        "                data_test[\"audio\"].append(os.path.join(test_dir, f))\n",
        "                data_test[\"label\"].append(model.config.label2id[label_name])\n",
        "\n",
        "    return data_train, data_test\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/processed_audios\"\n",
        "train_data, test_data = load_audio_split(base_dir)\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "test_dataset = Dataset.from_dict(test_data)\n",
        "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
        "\n",
        "print(f\"âœ… Loaded {len(train_dataset)} train and {len(test_dataset)} test samples.\")\n"
      ],
      "metadata": {
        "id": "MTvCWx00szba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ASTFeatureExtractor, AutoModelForAudioClassification\n",
        "import torch, librosa, numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# --- Load correct feature extractor and model ---\n",
        "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
        "feature_extractor = ASTFeatureExtractor.from_pretrained(model_name)\n",
        "model = AutoModelForAudioClassification.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "qYSWjdOQs39j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ASTFeatureExtractor, AutoModelForAudioClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Find hidden size safely ---\n",
        "try:\n",
        "    hidden_size = model.classifier.dense.in_features  # if it's an ASTMLPHead\n",
        "except AttributeError:\n",
        "    # Fallback if classifier is already Sequential or changed structure\n",
        "    hidden_size = model.classifier[0].in_features if isinstance(model.classifier, nn.Sequential) else model.classifier.in_features\n",
        "\n",
        "# --- Replace classifier for 2 labels ---\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(hidden_size, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, 2)\n",
        ")\n",
        "\n",
        "# --- Label mappings ---\n",
        "label2id = {\"NonViolence\": 0, \"Violence\": 1}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "model.config.num_labels = 2\n",
        "model.config.label2id = label2id\n",
        "model.config.id2label = id2label\n",
        "\n",
        "print(\"âœ… Replaced classifier head with 2-class Sequential layer\")\n",
        "print(model.classifier)\n"
      ],
      "metadata": {
        "id": "Nf0EW7Ses6PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "# If you already created your dataset, skip this. Otherwise, recreate:\n",
        "# dataset = load_dataset(\"path/to/your/train_test_data\", split=[\"train\", \"test\"])\n",
        "# or, if you manually built it:\n",
        "# dataset = DatasetDict({\"train\": train_ds, \"test\": test_ds})\n",
        "\n",
        "def preprocess_function(batch):\n",
        "    audios = []\n",
        "    for path in batch[\"audio\"]:\n",
        "        # Load with original sampling rate\n",
        "        arr, sr = librosa.load(path, sr=None, mono=True)\n",
        "        # Resample if needed\n",
        "        if sr != 16000:\n",
        "            arr = librosa.resample(arr.astype(np.float32), orig_sr=sr, target_sr=16000)\n",
        "        else:\n",
        "            arr = arr.astype(np.float32)\n",
        "\n",
        "        # Pad super short clips (AST needs window â‰¥400)\n",
        "        if len(arr) < 400:\n",
        "            arr = np.pad(arr, (0, 400 - len(arr)))\n",
        "        audios.append(arr)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        audios,\n",
        "        sampling_rate=16000,\n",
        "        padding=\"max_length\",\n",
        "        max_length=160000,   # ~10 s\n",
        "        truncation=True,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    return {\"input_values\": inputs[\"input_values\"], \"label\": batch[\"label\"]}\n",
        "\n",
        "dataset = dataset.map(preprocess_function, batched=True, batch_size=4)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_values\", \"label\"])\n",
        "print(\"âœ… Dataset ready for training\")\n"
      ],
      "metadata": {
        "id": "4_4vulxts9eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, set_seed\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "set_seed(42)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./violence_audio_model\",\n",
        "    do_eval=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False,     # âœ… disabled for compatibility\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=feature_extractor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer initialized successfully. Ready to train!\")\n"
      ],
      "metadata": {
        "id": "M5PNA1Kas_mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "Zx7pbjxjtBg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y wandb"
      ],
      "metadata": {
        "id": "i9SyXgETtDAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate\n",
        "from accelerate import Accelerator\n",
        "Accelerator().free_memory()\n",
        "\n",
        "import gc, torch\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GeX2Oj6ctIVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, set_seed\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "set_seed(42)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./violence_audio_model\",\n",
        "    do_eval=False,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=feature_extractor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer reinitialized successfully. Ready to train!\")\n"
      ],
      "metadata": {
        "id": "u9nNzbRytM-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "7DSw0haPv9m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fine-tuned model\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation metrics nicely\n",
        "print(\"ðŸ“Š Evaluation Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "# Optional â€” just show accuracy clearly\n",
        "print(\"\\nâœ… Model Accuracy:\", eval_results[\"eval_accuracy\"])"
      ],
      "metadata": {
        "id": "XGXh3NMTtRcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/drive/MyDrive/audio_violence_detection_model\"\n",
        "\n",
        "# Save model and config\n",
        "model.save_pretrained(save_dir)\n",
        "\n",
        "# Also save the feature extractor (important for loading later)\n",
        "feature_extractor.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"âœ… Audio model saved successfully at: {save_dir}\")\n"
      ],
      "metadata": {
        "id": "PoIJSAd0wHQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# âœ… Get predictions on the test set\n",
        "predictions = trainer.predict(dataset[\"test\"])\n",
        "\n",
        "# Extract predicted labels (highest probability)\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# True labels\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# âœ… Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Define label names (order: [NonViolence, Violence])\n",
        "labels = [\"NonViolence\", \"Violence\"]\n",
        "\n",
        "# âœ… Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=\"Blues\", values_format='d')\n",
        "\n",
        "plt.title(\"Confusion Matrix: Violence vs Non-Violence Audio\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A5ZjIxDEtTee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# ðŸŽ¯ Give one audio path manually\n",
        "audio_path = \"/content/drive/MyDrive/processed_audios/test/Violence/angry_022_V0.wav\"\n",
        "\n",
        "# Label mapping â€” must match training order\n",
        "id2label = {0: \"NonViolence\", 1: \"Violence\"}\n",
        "\n",
        "# âœ… Function to predict one audio file\n",
        "def predict_audio_label(audio_path):\n",
        "    waveform, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        waveform,\n",
        "        sampling_rate=sr,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Move to GPU if available\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        pred_id = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_id].item()\n",
        "\n",
        "    label = id2label[pred_id]\n",
        "    return label, confidence\n",
        "\n",
        "# âœ… Run prediction\n",
        "label, confidence = predict_audio_label(audio_path)\n",
        "\n",
        "print(f\"ðŸŽ§ Audio: {audio_path}\")\n",
        "print(f\"â†’ Predicted Label: {label}\")\n",
        "print(f\"â†’ Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "id": "Vekt-TlVtWWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# ðŸŽ¯ Give one audio path manually\n",
        "audio_path = \"/content/drive/MyDrive/processed_audios/test/NonViolence/angry_017_NV1.wav\"\n",
        "\n",
        "# Label mapping â€” must match training order\n",
        "id2label = {0: \"NonViolence\", 1: \"Violence\"}\n",
        "\n",
        "# âœ… Function to predict one audio file\n",
        "def predict_audio_label(audio_path):\n",
        "    waveform, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        waveform,\n",
        "        sampling_rate=sr,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Move to GPU if available\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        pred_id = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_id].item()\n",
        "\n",
        "    label = id2label[pred_id]\n",
        "    return label, confidence\n",
        "\n",
        "# âœ… Run prediction\n",
        "label, confidence = predict_audio_label(audio_path)\n",
        "\n",
        "print(f\"ðŸŽ§ Audio: {audio_path}\")\n",
        "print(f\"â†’ Predicted Label: {label}\")\n",
        "print(f\"â†’ Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "id": "FfYNsczctXEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# ðŸŽ¯ Give one audio path manually\n",
        "audio_path = \"/content/drive/MyDrive/processed_audios/test/Violence/angry_034_V0.wav\"\n",
        "\n",
        "# Label mapping â€” must match training order\n",
        "id2label = {0: \"NonViolence\", 1: \"Violence\"}\n",
        "\n",
        "# âœ… Function to predict one audio file\n",
        "def predict_audio_label(audio_path):\n",
        "    waveform, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        waveform,\n",
        "        sampling_rate=sr,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Move to GPU if available\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        pred_id = torch.argmax(probs, dim=-1).item()\n",
        "        confidence = probs[0][pred_id].item()\n",
        "\n",
        "    label = id2label[pred_id]\n",
        "    return label, confidence\n",
        "\n",
        "# âœ… Run prediction\n",
        "label, confidence = predict_audio_label(audio_path)\n",
        "\n",
        "print(f\"ðŸŽ§ Audio: {audio_path}\")\n",
        "print(f\"â†’ Predicted Label: {label}\")\n",
        "print(f\"â†’ Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "id": "RPpSXesttcP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, librosa, numpy as np, subprocess, os, uuid\n",
        "from transformers import (\n",
        "    VideoMAEForVideoClassification,\n",
        "    AutoModelForAudioClassification,\n",
        "    ASTFeatureExtractor,\n",
        ")\n",
        "from decord import VideoReader, cpu\n",
        "import torchvision.transforms as T\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import uuid\n",
        "from IPython.display import HTML, Audio, display\n",
        "from base64 import b64encode\n"
      ],
      "metadata": {
        "id": "L6a0qg8EzTNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_MODEL_PATH = \"/content/drive/MyDrive/video_violence_detection_model\"   # saved with save_pretrained()\n",
        "AUDIO_MODEL_PATH = \"/content/drive/MyDrive/audio_violence_detection_model\"   # saved with save_pretrained()\n",
        "TEST_DIR = \"/content/violence_dataset_split/test\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"âœ… Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "qlyqe9x1M3RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_model = VideoMAEForVideoClassification.from_pretrained(VIDEO_MODEL_PATH)\n",
        "video_model.to(DEVICE)\n",
        "video_model.eval()\n",
        "print(\" Video model loaded successfully\")"
      ],
      "metadata": {
        "id": "7IFhHx0NM5-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = ASTFeatureExtractor.from_pretrained(AUDIO_MODEL_PATH)\n",
        "audio_model = AutoModelForAudioClassification.from_pretrained(AUDIO_MODEL_PATH)\n",
        "audio_model.to(DEVICE)\n",
        "audio_model.eval()\n",
        "print(\"Audio model loaded successfully\")\n",
        "\n",
        "# Label mapping (must match training)\n",
        "id2label = {0: \"NonViolence\", 1: \"Violence\"}\n",
        "label2id = {\"NonViolence\": 0, \"Violence\": 1}"
      ],
      "metadata": {
        "id": "m6EFG4g4M81X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def has_audio(video_path):\n",
        "    \"\"\"Check if the video has an audio stream.\"\"\"\n",
        "    cmd = [\n",
        "        \"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"a\",\n",
        "        \"-show_entries\", \"stream=codec_type\", \"-of\", \"csv=p=0\", video_path\n",
        "    ]\n",
        "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    return bool(result.stdout.strip())\n",
        "\n",
        "def extract_audio_from_video(video_path):\n",
        "    \"\"\"Extracts audio if present.\"\"\"\n",
        "    temp_audio_path = f\"/content/temp_audio_{uuid.uuid4().hex}.wav\"\n",
        "    command = [\n",
        "        \"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\",\n",
        "        temp_audio_path, \"-y\", \"-loglevel\", \"error\"\n",
        "    ]\n",
        "    subprocess.run(command)\n",
        "    return temp_audio_path if os.path.exists(temp_audio_path) else None\n"
      ],
      "metadata": {
        "id": "XBQFuukoVp7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video_frames(video_path, num_frames=16, frame_size=(224,224)):\n",
        "    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "    total = len(vr)\n",
        "    frame_idx = torch.linspace(0, total-1, num_frames).long()\n",
        "    frames = vr.get_batch(frame_idx).float()/255.0\n",
        "    frames = frames.permute(0,3,1,2)\n",
        "    transform = T.Compose([\n",
        "        T.Resize(frame_size),\n",
        "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "    frames = torch.stack([transform(f) for f in frames])\n",
        "    return frames.unsqueeze(0)"
      ],
      "metadata": {
        "id": "7ZYbfxQiST2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio_features(audio_path):\n",
        "    waveform, sr = librosa.load(audio_path, sr=16000)\n",
        "    return feature_extractor(\n",
        "        waveform, sampling_rate=sr,\n",
        "        return_tensors=\"pt\", padding=True, truncation=True\n",
        "    )\n"
      ],
      "metadata": {
        "id": "n8XDGUy9SX2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_fusion(video_path, alpha=0.6):\n",
        "    video_frames = load_video_frames(video_path).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        video_logits = video_model(video_frames).logits\n",
        "\n",
        "    if has_audio(video_path):\n",
        "        audio_path = extract_audio_from_video(video_path)\n",
        "        if audio_path:\n",
        "            audio_inputs = load_audio_features(audio_path)\n",
        "            audio_inputs = {k:v.to(DEVICE) for k,v in audio_inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                audio_logits = audio_model(**audio_inputs).logits\n",
        "            os.remove(audio_path)\n",
        "            fused_logits = alpha*video_logits + (1-alpha)*audio_logits\n",
        "            probs = torch.nn.functional.softmax(fused_logits, dim=-1)\n",
        "            return torch.argmax(probs, dim=-1).item()\n",
        "    # fallback: no audio\n",
        "    probs = torch.nn.functional.softmax(video_logits, dim=-1)\n",
        "    return torch.argmax(probs, dim=-1).item()"
      ],
      "metadata": {
        "id": "NfZlcgzFUIRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels, pred_labels = [], []\n",
        "\n",
        "for cls in [\"NonViolence\",\"Violence\"]:\n",
        "    folder = os.path.join(TEST_DIR, cls)\n",
        "    label = label2id[cls]\n",
        "    for file in os.listdir(folder):\n",
        "        if file.lower().endswith((\".mp4\",\".avi\",\".mov\")):\n",
        "            path = os.path.join(folder,file)\n",
        "            try:\n",
        "                pred = predict_fusion(path, alpha=0.6)\n",
        "                true_labels.append(label)\n",
        "                pred_labels.append(pred)\n",
        "                print(f\"{file}: True={cls}, Pred={id2label[pred]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error processing {file}: {e}\")\n"
      ],
      "metadata": {
        "id": "HYrwymPXULaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_curve, auc, classification_report\n",
        ")\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gEC4OvSiZGsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "print(\" Evaluation Metrics:\")\n",
        "print(f\"Accuracy : {accuracy*100:.2f}%\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall   : {recall:.3f}\")\n",
        "print(f\"F1 Score : {f1:.3f}\")"
      ],
      "metadata": {
        "id": "1GTRKLJ4ZK6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=[\"NonViolence\", \"Violence\"]))"
      ],
      "metadata": {
        "id": "YrDsgnEbZPyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "labels = [\"NonViolence\", \"Violence\"]\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"YlGnBu\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Fusion Model Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "th5dLkEVZWWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
        "    \"Fusion Model\": [f\"{accuracy*100:.2f}%\", f\"{precision:.3f}\", f\"{recall:.3f}\", f\"{f1:.3f}\"]\n",
        "}\n",
        "metrics_df = pd.DataFrame(data)\n",
        "display(metrics_df.style.set_caption(\"Fusion Model Performance Summary\"))\n"
      ],
      "metadata": {
        "id": "-2ILSAEGZeaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = np.sum(np.array(true_labels) == np.array(pred_labels))\n",
        "incorrect = len(true_labels) - correct\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.pie(\n",
        "    [correct, incorrect],\n",
        "    labels=[\"Correct Predictions\", \"Incorrect Predictions\"],\n",
        "    autopct='%1.1f%%', startangle=140,\n",
        "    colors=[\"#4CAF50\", \"#F44336\"]\n",
        ")\n",
        "plt.title(\"Fusion Model Prediction Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "npwy4lz0Z0gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_video(video_path, width=480):\n",
        "    \"\"\"Display video inline in Colab.\"\"\"\n",
        "    mp4 = open(video_path, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return f\"\"\"\n",
        "        <video width={width} controls>\n",
        "            <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "        </video>\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "gqIeJFyAXFFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_clip(video_path, alpha=0.6):\n",
        "    print(f\" Processing: {video_path}\\n\")\n",
        "\n",
        "    # Display video\n",
        "    display(HTML(display_video(video_path, width=480)))\n",
        "\n",
        "    # ---- Video Prediction ----\n",
        "    video_frames = load_video_frames(video_path).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        video_logits = video_model(video_frames).logits\n",
        "\n",
        "    # ---- Audio Handling ----\n",
        "    fused_logits = video_logits\n",
        "    audio_available = has_audio(video_path)\n",
        "    if audio_available:\n",
        "        audio_path = extract_audio_from_video(video_path)\n",
        "        if audio_path:\n",
        "            display(Audio(audio_path, autoplay=False))\n",
        "            audio_inputs = load_audio_features(audio_path)\n",
        "            audio_inputs = {k: v.to(DEVICE) for k, v in audio_inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                audio_logits = audio_model(**audio_inputs).logits\n",
        "            # Weighted fusion\n",
        "            fused_logits = alpha * video_logits + (1 - alpha) * audio_logits\n",
        "            os.remove(audio_path)\n",
        "        else:\n",
        "            print(\" Audio extraction failed â€” using video model only.\")\n",
        "    else:\n",
        "        print(\" No audio stream detected â€” using video model only.\")\n",
        "\n",
        "    # ---- Prediction ----\n",
        "    probs = torch.nn.functional.softmax(fused_logits, dim=-1)\n",
        "    pred_id = torch.argmax(probs, dim=-1).item()\n",
        "    confidence = probs[0, pred_id].item()\n",
        "    label = id2label[pred_id]\n",
        "\n",
        "    print(f\"\\n Predicted Label: {label}\")\n",
        "    print(f\" Confidence: {confidence:.2f}\")\n",
        "    if audio_available:\n",
        "        print(f\" Video Weight: {alpha},  Audio Weight: {1 - alpha}\")\n",
        "\n",
        "    return label, confidence"
      ],
      "metadata": {
        "id": "OzZboGGhSctQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_clip_path = \"/content/drive/MyDrive/Colab Notebooks/Non_Violence1.mp4\"  # ðŸ‘ˆ your clip (with audio + video)\n",
        "label, conf = predict_single_clip(video_clip_path, alpha=0.6)"
      ],
      "metadata": {
        "id": "ij5RENaWSef8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_clip_path = \"/content/drive/MyDrive/Colab Notebooks/Violence1.mp4\" # ðŸ‘ˆ your clip (with audio + video)\n",
        "label, conf = predict_single_clip(video_clip_path, alpha=0.6)"
      ],
      "metadata": {
        "id": "PvnyJ2N_TNUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_clip_path = \"/content/drive/MyDrive/Colab Notebooks/Non_Violence2.mp4\" # ðŸ‘ˆ your clip (with audio + video)\n",
        "label, conf = predict_single_clip(video_clip_path, alpha=0.6)"
      ],
      "metadata": {
        "id": "dO6Tf5wWXbPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_clip_path = \"/content/drive/MyDrive/Colab Notebooks/Violence2.mp4\" # ðŸ‘ˆ your clip (with audio + video)\n",
        "label, conf = predict_single_clip(video_clip_path, alpha=0.6)"
      ],
      "metadata": {
        "id": "uujKc4DIXpH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}